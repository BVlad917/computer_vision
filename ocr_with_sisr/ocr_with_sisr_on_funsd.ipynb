{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9963d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz\n",
    "from torchvision import io\n",
    "from pytesseract import Output\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from pytesseract import pytesseract\n",
    "import torchvision.ops.boxes as bops\n",
    "from shapely.geometry import Polygon\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "PATH_2_TESSERACT = \"/usr/bin/tesseract\"\n",
    "pytesseract.tesseract_cmd = PATH_2_TESSERACT\n",
    "\n",
    "sys.path.append(\"/data/local/data/personal/github_repos/sr_server/\")\n",
    "\n",
    "from sisr import get_sisr_forward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087fa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook formatting\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c14c7",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OcrBbox:\n",
    "    \"\"\"Class for keeping track of an OCR bounding box.\"\"\"\n",
    "    bbox: tuple  # 4-tuple with (x_left, y_upper, x_right, y_lower)\n",
    "    text: str  # the text in the prediciton/ground truth\n",
    "    confidence: int = None  # confidence of prediction (leave to None for ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d66507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FUNSDDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, sr_fn=None):\n",
    "        super().__init__()\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(images_path, \"*.png\")))\n",
    "        self.annotation_paths = sorted(glob.glob(os.path.join(annotations_path, \"*.json\")))\n",
    "        self.sr_fn = sr_fn\n",
    "        \n",
    "        # verify corectness of data\n",
    "        assert len(self.image_paths) == len(self.annotation_paths), \"ERROR: Must have 1-to-1 image-to-annotation correspondence\"\n",
    "        image_indexes = np.array([os.path.basename(t).split('.')[0] for t in self.image_paths])\n",
    "        annotation_indexes = np.array([os.path.basename(t).split('.')[0] for t in self.annotation_paths])\n",
    "        assert np.all(image_indexes == annotation_indexes), \"ERROR: At least one image does not have an annotation JSON file.\"\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # read bounding boxes with their corresonding text predictions\n",
    "        json_data = json.load(open(self.annotation_paths[idx]))\n",
    "        bboxes = [OcrBbox(text=w[\"text\"], bbox=tuple(w[\"box\"])) for p in json_data[\"form\"] for w in p[\"words\"]]\n",
    "        if self.sr_fn:\n",
    "            image = self.sr_fn(image)\n",
    "            for box in bboxes:\n",
    "                box.bbox = tuple(c * 4 for c in box.bbox)\n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483cb06",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tesseract(img):\n",
    "    \"\"\"\n",
    "    Run Google Tesseract OCR on the given image and return all the information provided by the OCR system.\n",
    "    :param img: np array; RGB format image\n",
    "    :return: a dictionary with the following keys:\n",
    "        - \"text\": list of strings; the words predicted by the OCR system\n",
    "        - \"confidence\": list of floats: confidence for each predicted word, provided by the OCR system\n",
    "        - \"box\": list of 4-tuples containing the bounding box in the format (x0, y0, x1, y1). Defines the\n",
    "        upper left and lower right points\n",
    "    \"\"\"\n",
    "    d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "    texts, confidences, boxes = [], [], []\n",
    "    num_boxes = len(d['level'])\n",
    "    for i in range(num_boxes):\n",
    "        text = d['text'][i]\n",
    "        conf = d['conf'][i] / 100. if d['conf'][i] >= 0 else d['conf'][i]  # normalized confidence\n",
    "        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "        (x_left, y_top, x_right, y_down) = (x, y, x + w, y + h)\n",
    "\n",
    "        texts.append(text)\n",
    "        confidences.append(conf)\n",
    "        boxes.append((x_left, y_top, x_right, y_down))\n",
    "    return {\"text\": texts, \"confidence\": confidences, \"box\": boxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tesseract_output_2_ocr_box(tesseract_out):\n",
    "    \"\"\"Convert Google Tesseract OCR output to custom OcrBox format.\"\"\"\n",
    "    return [OcrBbox(bbox=b, text=t, confidence=c) \n",
    "            for t, c, b in zip(tesseract_out[\"text\"], tesseract_out[\"confidence\"], tesseract_out[\"box\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_bbox_to_polygon(bbox):\n",
    "    \"\"\"Convert a COCO-format bounding box (defined by 2 points: upper-left and lower-right) to a shapely polygon.\"\"\"\n",
    "    assert len(bbox) == 4\n",
    "    upper_left = bbox[:2]\n",
    "    lower_right = bbox[2:]\n",
    "    upper_right = [lower_right[0], upper_left[1]]\n",
    "    lower_left = [upper_left[0], lower_right[1]]\n",
    "    return Polygon(shell=[upper_left, upper_right, lower_right, lower_left])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0adf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bbox_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Vectorized implementation which calculates the IoU between two lists of bounding boxes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : list (of length N) of 4-tuples\n",
    "        Format : [(x1, y1, x2, y2), b2, b3, ...]\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : list (of length M) of 4-tuples\n",
    "        Format : [(x1, y1, x2, y2), b2, b3, ...]\n",
    "        The (x, y) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PyTorch tensor of shape NxM. Each entry (i, j) contains the IoU between\n",
    "    bounding box i from bb1 and bounding box j from bb2.\n",
    "    \"\"\"\n",
    "    box1 = torch.tensor(bb1, dtype=torch.float)\n",
    "    box2 = torch.tensor(bb2, dtype=torch.float)\n",
    "    iou = bops.box_iou(box1, box2)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pred_to_gt_bboxes(gt_bboxes, pred_bboxes):\n",
    "    \"\"\"\n",
    "    Match a list of predicted bounding boxes to a list of ground-truth bounding boxes 1-to-1. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_bboxes : list (of length M) of ground-truth OcrWord bounding boxes\n",
    "    pred_bboxes : list (of length N) of predicted OcrWord bounding boxes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of dictionaries. This represents the matching from GT to PRED in OCR rectangles.\n",
    "        Keys : {\"gt\", \"pred\", \"iou\"}\n",
    "        The value at the \"gt\" key contains a ground-truth OcrWord bounding box\n",
    "        The value at the \"pred\" key contains a predicted OcrWord bounding box\n",
    "        The value at the \"iou\" key contains the IoU between these two matched bounding boxes\n",
    "    \"\"\"\n",
    "    gt_bboxes_points = [b.bbox for b in gt_bboxes]\n",
    "    pred_bboxes_points = [b.bbox for b in pred_bboxes]\n",
    "    iou_matrix = calculate_bbox_iou(gt_bboxes_points, pred_bboxes_points)\n",
    "    max_iou, max_iou_index = torch.max(iou_matrix, dim=1)\n",
    "    matched_boxes = [{\"gt\": gt_bboxes[gt_idx], \"pred\": pred_bboxes[pred_idx], \"iou\": iou.item()} \n",
    "                     for gt_idx, (pred_idx, iou) in enumerate(zip(max_iou_index, max_iou))]\n",
    "    return matched_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text):\n",
    "    \"\"\"\n",
    "    Pre-process a text before using it for OCR evaluation (e.g., use lower \n",
    "    case letters since knowing which letters are capitalized is not vital information)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ocr_system(dset, ocr_fn):\n",
    "    \"\"\"\n",
    "    Run a given OCR system on all the images in the given dataset and evaluate the performance\n",
    "    of the OCR system.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dset : PyTorch dataset, items should have the following keys: \n",
    "        \"image\": NumPy array, RGB format, channels last representation\n",
    "        \"bboxes\": list of OcrBbox instances; The ground-truth bounding boxes from the given image\n",
    "    ocr_fn : function which runs an OCR system on a image. Should return a dictionary with the\n",
    "    following keys:\n",
    "        \"text\": list of strings with the ground truth text from a bounding box\n",
    "        \"box\": list of tuples of 4 integers (so 2 2D points) representing the rectangle of the bounding box\n",
    "        \"confidence\": list of floats representing the confidence of the i'th detection\n",
    "        All the lists from the above keys have the same length.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of lists. Each element list l_i contains the similarity scores from the ground-truth\n",
    "    bounding boxes in the i'th image.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for dset_item in tqdm(dset):\n",
    "        # get the current image and its GT bounding boxes\n",
    "        image = dset_item[\"image\"]\n",
    "        bboxes_gt = dset_item[\"bboxes\"]\n",
    "        \n",
    "        # run the OCR model on the image\n",
    "        bboxes_dict = ocr_fn(image)\n",
    "        bboxes_pred = tesseract_output_2_ocr_box(bboxes_dict)\n",
    "        \n",
    "        # match the GT bounding boxes with the PRED bounding boxes, apply filter, and sort \n",
    "        matching = match_pred_to_gt_bboxes(gt_bboxes=bboxes_gt, pred_bboxes=bboxes_pred)\n",
    "        filtered_matching = [m for m in matching if len(m[\"gt\"].text.strip()) and len(m[\"pred\"].text.strip())]\n",
    "        \n",
    "        img_level_similarities = []\n",
    "        for m in filtered_matching:\n",
    "            gt_bbox = m[\"gt\"]\n",
    "            pred_bbox = m[\"pred\"]\n",
    "            iou = m[\"iou\"]\n",
    "            \n",
    "            gt_text, pred_text = gt_bbox.text, pred_bbox.text\n",
    "            gt_text, pred_text = pre_process_text(gt_text), pre_process_text(pred_text)\n",
    "            \n",
    "            # consider similarity ratio only if the IoU is above a threshold and thus the GT text was recognized\n",
    "            img_level_similarities.append(fuzz.ratio(pred_text, gt_text) / 100. if iou > IOU_THRESHOLD else 0)\n",
    "        similarities.append(img_level_similarities)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5df4f2",
   "metadata": {},
   "source": [
    "### Dataset and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "funsd_train_dir = \"/data/local/data/personal/ocr_data/funsd_dataset/training_data/\"\n",
    "funsd_test_dir = \"/data/local/data/personal/ocr_data/funsd_dataset/testing_data\"\n",
    "\n",
    "train_dset = FUNSDDataset(images_path=os.path.join(funsd_train_dir, \"images\"), \n",
    "                          annotations_path=os.path.join(funsd_train_dir, \"annotations\"))\n",
    "test_dset = FUNSDDataset(images_path=os.path.join(funsd_test_dir, \"images\"), \n",
    "                          annotations_path=os.path.join(funsd_test_dir, \"annotations\"))\n",
    "\n",
    "IOU_THRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f14324",
   "metadata": {},
   "source": [
    "### Visualize ground truth bounding boxes vs. predicted bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_item = train_dset[2]\n",
    "image = dset_item[\"image\"]\n",
    "bboxes_gt = dset_item[\"bboxes\"]\n",
    "\n",
    "# add gt bounding boxes\n",
    "copy_gt = np.copy(image)\n",
    "for bbox in bboxes_gt:\n",
    "    cv2.rectangle(copy_gt, bbox.bbox[:2], bbox.bbox[2:], (0, 255, 0), 1)\n",
    "    \n",
    "# add pred bounding boxes\n",
    "bboxes_dict = run_tesseract(image)\n",
    "bboxes_pred = tesseract_output_2_ocr_box(bboxes_dict)\n",
    "bboxes_pred = [b for b in bboxes_pred if len(b.text.strip())]\n",
    "copy_pred = np.copy(image)\n",
    "for bbox in bboxes_pred:\n",
    "    cv2.rectangle(copy_pred, bbox.bbox[:2], bbox.bbox[2:], (0, 255, 0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f4d3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(copy_gt)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"GT bounding boxes\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(copy_pred)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"PRED bounding boxes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256ff93",
   "metadata": {},
   "source": [
    "### Visualize how the matching algorithm works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9e8a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset_item = train_dset[1]\n",
    "image = dset_item[\"image\"]\n",
    "bboxes_gt = dset_item[\"bboxes\"]\n",
    "\n",
    "# run the OCR model on the image\n",
    "bboxes_dict = run_tesseract(image)\n",
    "bboxes_pred = tesseract_output_2_ocr_box(bboxes_dict)\n",
    "\n",
    "# match the GT bounding boxes with the PRED bounding boxes, apply filter, and sort \n",
    "matching = match_pred_to_gt_bboxes(gt_bboxes=bboxes_gt, pred_bboxes=bboxes_pred)\n",
    "filtered_matching = [m for m in matching if len(m[\"gt\"].text.strip()) and len(m[\"pred\"].text.strip())]  # remove spaces and tabs\n",
    "filtered_matching = sorted(filtered_matching, key=lambda x: x[\"iou\"], reverse=True)\n",
    "\n",
    "for m in filtered_matching:\n",
    "    gt_bbox = m[\"gt\"]\n",
    "    pred_bbox = m[\"pred\"]\n",
    "    iou = m[\"iou\"]\n",
    "\n",
    "    gt_points, pred_points = gt_bbox.bbox, pred_bbox.bbox\n",
    "    gt_text, pred_text = gt_bbox.text, pred_bbox.text\n",
    "    gt_text, pred_text = pre_process_text(gt_text), pre_process_text(pred_text)\n",
    "    \n",
    "    crop_gt = image[gt_points[1] : gt_points[3] + 1, gt_points[0] : gt_points[2] + 1, :]\n",
    "    crop_pred = image[pred_points[1] : pred_points[3] + 1, pred_points[0] : pred_points[2] + 1, :]\n",
    "    \n",
    "    plt.figure(figsize=(4, 2))\n",
    "    ax1, ax2 = plt.subplot(1, 2, 1), plt.subplot(1, 2, 2)\n",
    "    \n",
    "    ax1.imshow(crop_gt)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"GT Bounding Box\", fontdict={\"fontsize\": 10})\n",
    "\n",
    "    ax2.imshow(crop_pred)\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.set_title(\"PRED Bounding Box\", fontdict={\"fontsize\": 10})\n",
    "    \n",
    "    plt.subplots_adjust(top=0.6, wspace=0.5)\n",
    "    plt.suptitle(f'GT: \"{gt_text}\"\\nPRED: \"{pred_text}\"\\nSIMILARITY: {(fuzz.ratio(gt_text, pred_text) / 100. if iou > IOU_THRESHOLD else 0):.2f}', fontsize=11)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574159a6",
   "metadata": {},
   "source": [
    "### Evaluate the OCR system on the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a251513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities = evaluate_ocr_system(test_dset, run_tesseract)\n",
    "similarities_flattened = [s for similarity_list in similarities for s in similarity_list]  # flatten to list of word-level similarities\n",
    "print(f\"Mean similarity score: {np.mean(similarities_flattened):.2f}\")\n",
    "print(f\"Median similarity score: {np.median(similarities_flattened):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcbe75",
   "metadata": {},
   "source": [
    "### Run super-resolution on the FUNSD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_ir_fn = get_sisr_forward_fn(model_name=\"swin_ir\", use_tiling=False)\n",
    "super_test_dset = FUNSDDataset(images_path=os.path.join(funsd_test_dir, \"images\"), \n",
    "                               annotations_path=os.path.join(funsd_test_dir, \"annotations\"), \n",
    "                               sr_fn=swin_ir_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f614497",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_similarities = evaluate_ocr_system(super_test_dset, run_tesseract)\n",
    "super_similarities_flattened = [s for similarity_list in super_similarities for s in similarity_list]  # flatten to list of word-level similarities\n",
    "print(f\"Mean similarity score: {np.mean(super_similarities_flattened):.2f}\")\n",
    "print(f\"Median similarity score: {np.median(super_similarities_flattened):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53890ebf",
   "metadata": {},
   "source": [
    "### See how the OCR system behaves in general on the super-resoluted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc55da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset_item = super_test_dset[1]\n",
    "image = dset_item[\"image\"]\n",
    "bboxes_gt = dset_item[\"bboxes\"]\n",
    "\n",
    "# run the OCR model on the image\n",
    "bboxes_dict = run_tesseract(image)\n",
    "bboxes_pred = tesseract_output_2_ocr_box(bboxes_dict)\n",
    "\n",
    "# match the GT bounding boxes with the PRED bounding boxes, apply filter, and sort \n",
    "matching = match_pred_to_gt_bboxes(gt_bboxes=bboxes_gt, pred_bboxes=bboxes_pred)\n",
    "filtered_matching = [m for m in matching if len(m[\"gt\"].text.strip()) and len(m[\"pred\"].text.strip())]  # remove spaces and tabs\n",
    "filtered_matching = sorted(filtered_matching, key=lambda x: x[\"iou\"], reverse=True)\n",
    "\n",
    "for m in filtered_matching:\n",
    "    gt_bbox = m[\"gt\"]\n",
    "    pred_bbox = m[\"pred\"]\n",
    "    iou = m[\"iou\"]\n",
    "\n",
    "    gt_points, pred_points = gt_bbox.bbox, pred_bbox.bbox\n",
    "    gt_text, pred_text = gt_bbox.text, pred_bbox.text\n",
    "    gt_text, pred_text = pre_process_text(gt_text), pre_process_text(pred_text)\n",
    "    \n",
    "    crop_gt = image[gt_points[1] : gt_points[3] + 1, gt_points[0] : gt_points[2] + 1, :]\n",
    "    crop_pred = image[pred_points[1] : pred_points[3] + 1, pred_points[0] : pred_points[2] + 1, :]\n",
    "    \n",
    "    plt.figure(figsize=(4, 2))\n",
    "    ax1, ax2 = plt.subplot(1, 2, 1), plt.subplot(1, 2, 2)\n",
    "    \n",
    "    ax1.imshow(crop_gt)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"GT Bounding Box\", fontdict={\"fontsize\": 10})\n",
    "\n",
    "    ax2.imshow(crop_pred)\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.set_title(\"PRED Bounding Box\", fontdict={\"fontsize\": 10})\n",
    "    \n",
    "    plt.subplots_adjust(top=0.6, wspace=0.5)\n",
    "    plt.suptitle(f'GT: \"{gt_text}\"\\nPRED: \"{pred_text}\"\\nSIMILARITY: {(fuzz.ratio(gt_text, pred_text) / 100. if iou > IOU_THRESHOLD else 0):.2f}', fontsize=11)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf72e60",
   "metadata": {},
   "source": [
    "### See the differences visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ff896",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('bmh')\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "# Example data\n",
    "methods = ('NO SR', 'SR',)\n",
    "y_pos = (0.1, 0.3)\n",
    "performance = np.array([np.mean(similarities_flattened), np.mean(super_similarities_flattened)])\n",
    "\n",
    "ax.barh(y_pos, performance, align='center', height=0.1)\n",
    "# Add the values on the bars\n",
    "for i, v in enumerate(performance):\n",
    "    ax.text(v + 0.01, y_pos[i], str(round(v, 2)), va='center')\n",
    "\n",
    "ax.set_yticks(y_pos, labels=methods)\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Average Text Similarity')\n",
    "# ax.set_title('OCR performance on the FUNSD dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a5215",
   "metadata": {},
   "source": [
    "### See examples where SISR helps the text detector\n",
    "#### a.k.a: places where we now have new detections, which were not found by the OCR detector on the original images but are found on the super-resoluted images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe452ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_item = super_test_dset[-2]\n",
    "super_image = dset_item[\"image\"]\n",
    "bboxes_gt = dset_item[\"bboxes\"]\n",
    "\n",
    "# run the OCR model on the image\n",
    "bboxes_dict = run_tesseract(super_image)\n",
    "bboxes_pred = tesseract_output_2_ocr_box(bboxes_dict)\n",
    "\n",
    "# match the GT bounding boxes with the PRED bounding boxes, apply filter, and sort \n",
    "matching_super = match_pred_to_gt_bboxes(gt_bboxes=bboxes_gt, pred_bboxes=bboxes_pred)\n",
    "filtered_matching_super = [m for m in matching_super if len(m[\"gt\"].text.strip()) and len(m[\"pred\"].text.strip())]  # remove spaces and tabs\n",
    "filtered_matching_super = sorted(filtered_matching_super, key=lambda x: x[\"iou\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_item = test_dset[-2]\n",
    "original_image = dset_item[\"image\"]\n",
    "bboxes_gt = dset_item[\"bboxes\"]\n",
    "\n",
    "# run the OCR model on the image\n",
    "bboxes_dict = run_tesseract(original_image)\n",
    "bboxes_pred = tesseract_output_2_ocr_box(bboxes_dict)\n",
    "\n",
    "# match the GT bounding boxes with the PRED bounding boxes, apply filter, and sort \n",
    "matching = match_pred_to_gt_bboxes(gt_bboxes=bboxes_gt, pred_bboxes=bboxes_pred)\n",
    "filtered_matching = [m for m in matching if len(m[\"gt\"].text.strip()) and len(m[\"pred\"].text.strip())]  # remove spaces and tabs\n",
    "filtered_matching = sorted(filtered_matching, key=lambda x: x[\"iou\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ba971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, r in filtered_super_df[filtered_super_df[\"gt_box\"].apply(lambda b: b not in set(filtered_df[\"gt_box\"].values))].iterrows():\n",
    "    pred_box = r[\"pred_box\"]\n",
    "    pred_text = r[\"pred_text\"]\n",
    "    small_box = tuple(c // 4 for c in pred_box)\n",
    "    \n",
    "    plt.figure(figsize=(4, 2))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_image[small_box[1]:small_box[3], small_box[0]:small_box[2], :])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(super_image[pred_box[1]:pred_box[3], pred_box[0]:pred_box[2], :])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplots_adjust(top=0.8, wspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a8482",
   "metadata": {},
   "source": [
    "### See examples where SISR helps (or not) the text recognition\n",
    "#### a.k.a., places where the OCR text recogniton model outputs different characters after applying the super-resolution pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.DataFrame(data=filtered_matching)\n",
    "filtered_df[\"gt_box\"] = filtered_df[\"gt\"].apply(lambda t: t.bbox)\n",
    "filtered_df[\"gt_text\"] = filtered_df[\"gt\"].apply(lambda t: t.text)\n",
    "filtered_df[\"pred_box\"] = filtered_df[\"pred\"].apply(lambda t: t.bbox)\n",
    "filtered_df[\"pred_text\"] = filtered_df[\"pred\"].apply(lambda t: t.text)\n",
    "filtered_df = filtered_df.drop(columns=[\"gt\", \"pred\"])\n",
    "\n",
    "filtered_super_df = pd.DataFrame(data=filtered_matching_super)\n",
    "filtered_super_df[\"gt_box\"] = filtered_super_df[\"gt\"].apply(lambda t: tuple(c // 4 for c in t.bbox))\n",
    "filtered_super_df[\"gt_text\"] = filtered_super_df[\"gt\"].apply(lambda t: t.text)\n",
    "filtered_super_df[\"pred_box\"] = filtered_super_df[\"pred\"].apply(lambda t: t.bbox)\n",
    "filtered_super_df[\"pred_text\"] = filtered_super_df[\"pred\"].apply(lambda t: t.text)\n",
    "filtered_super_df = filtered_super_df.drop(columns=[\"gt\", \"pred\"])\n",
    "\n",
    "merged_df = pd.merge(left=filtered_df, right=filtered_super_df, on=\"gt_box\", suffixes=(\"_original\", \"_super\"))\n",
    "merged_df[\"gt_text\"] = merged_df[\"gt_text_original\"]\n",
    "merged_df = merged_df.drop(columns=[\"gt_text_original\", \"gt_text_super\"])\n",
    "new_column_order = [\"gt_box\", \"gt_text\", \"pred_box_original\", \"pred_text_original\", \"iou_original\", \"pred_box_super\", \"pred_text_super\", \"iou_super\"]\n",
    "merged_df = merged_df[new_column_order]\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92599f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, r in merged_df[merged_df.apply(lambda r: r[\"pred_text_original\"] != r[\"pred_text_super\"], axis=1)][:].iterrows():\n",
    "    pred_text_original = r[\"pred_text_original\"]\n",
    "    pred_text_super = r[\"pred_text_super\"]\n",
    "    \n",
    "    pred_box_original = r[\"pred_box_original\"]\n",
    "    pred_box_super = r[\"pred_box_super\"]\n",
    "    \n",
    "    gt_box = r[\"gt_box\"]\n",
    "    gt_text = r[\"gt_text\"]\n",
    "    \n",
    "    plt.figure(figsize=(4, 2))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_image[pred_box_original[1]:pred_box_original[3], pred_box_original[0]:pred_box_original[2], :])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"ORIGINAL\\n'{pred_text_original}'\", fontdict={\"fontsize\": 10})\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(super_image[pred_box_super[1]:pred_box_super[3], pred_box_super[0]:pred_box_super[2], :])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"SR\\n'{pred_text_super}'\", fontdict={\"fontsize\": 10})\n",
    "    \n",
    "    plt.subplots_adjust(top=0.8, wspace=0.5)\n",
    "    plt.suptitle(f\"ACTUAL TEXT: '{gt_text}'\", fontsize=11)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16e001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
